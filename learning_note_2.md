**《动手学深度学习》笔记2**  
***  
***Part1 卷积神经网络***  
>参数量的计算  
  
输入形状为r·r,输出形状为c·c（从卷积神经网络的知识可以得知，r和c并不是完全无关的）
假设为全连接层连接，输入的通道数x，输出的通道数y，核的大小kn·kn，填充特征padding = n，步长s，有偏置，则参数量为x·(r+2n-kn)·(r+2n-kn)·y+y.  
>LeNet  
  
LeNet的卷积层块交替使用卷积层和最大池化层，然后使用全连接层，来实现图像的分类。  
随着层数的加深，每层的大小在不断变小，但是通道数量不断增加，以此来避免信息的损失。  

>卷积神经网络进阶  

主要是深度卷积神经网络。  
从AlexNet开始。sigmoid函数在两级区间容易出现梯度消失的情况，ReLU函数的梯度保持不变，更容易产生梯度上的快速变化，收敛更快；同时ReLU函数在负半轴区间上为0，也起到了正则化、稀疏化的作用。但是AlexNet比较死板，不容易增加或者简化。  
由此，产生了更加方便的VGG模型和NiN（网络中的网络），NiN使用1x1的卷积层代替全连接层增加了非线性特征，也可以实现通道数的放缩，同时，相比全连接层参数也会比较少。
***  
***Part2 循环神经网络***  
>one-hot向量  

我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是N，每次字符对应一个从0到N−1的唯一的索引，则该字符的向量是一个长度为N的向量，若字符的索引是i，则该向量的第i个位置为1，其他位置为0。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。  
***  
***Part3 机器学习的应用***  
>机器翻译及相关技术  

>注意力机制与Seq2seq模型  

>Transformer

