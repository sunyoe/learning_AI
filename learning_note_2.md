**《动手学深度学习》笔记2**  
***  
***Part1 卷积神经网络***  
>参数量的计算  
  
输入形状为r·r,输出形状为c·c（从卷积神经网络的知识可以得知，r和c并不是完全无关的）
假设为全连接层连接，输入的通道数x，输出的通道数y，核的大小kn·kn，填充特征padding = n，步长s，有偏置，则参数量为x·(r+2n-kn)·(r+2n-kn)·y+y.  
>LeNet  
  
LeNet的卷积层块交替使用卷积层和最大池化层，然后使用全连接层，来实现图像的分类。  
随着层数的加深，每层的大小在不断变小，但是通道数量不断增加，以此来避免信息的损失。  

>卷积神经网络进阶  

主要是深度卷积神经网络。  
从AlexNet开始。sigmoid函数在两级区间容易出现梯度消失的情况，ReLU函数的梯度保持不变，更容易产生梯度上的快速变化，收敛更快；同时ReLU函数在负半轴区间上为0，也起到了正则化、稀疏化的作用。但是AlexNet比较死板，不容易增加或者简化。  
由此，产生了更加方便的VGG模型和NiN（网络中的网络），NiN使用1x1的卷积层代替全连接层增加了非线性特征，也可以实现通道数的放缩，同时，相比全连接层参数也会比较少。
***  
***Part2 循环神经网络***  
>one-hot向量  

我们需要将字符表示成向量，这里采用one-hot向量。假设词典大小是N，每次字符对应一个从0到N−1的唯一的索引，则该字符的向量是一个长度为N的向量，若字符的索引是i，则该向量的第i个位置为1，其他位置为0。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。  

>循环神经网络进阶  

利用循环神经网络的思路，诞生出了许多能够有效分析长语句并且给出预测结果的模型。  
基础的有GRU模型，LSTM模型，在基础模型的基础上，有多层的深度循环神经网络，还有能够同时分析正向和反向逻辑的双向循环神经网络。  
***  
***Part3 机器学习的应用***  
>机器翻译及相关技术  

在循环神经网络的基础上进行实现，问题在于，输入和输出通道不对应，例如从“I'm Chinese.”到“我是中国人。”，英文中大致有3个字，但是中文有5个字，这样字数是无法实现规定的。  
需要首先进行字典的录入，使用数据预处理和分词等形式以便更好地实现机器翻译。实现单词->ID->词向量，才能进入循环神经网络进行分析。  
最关键的是encoder-decoder模块，使用语义编码（实际上是hidden state）进行语义的交接，从而灵活地产生相同语义的词句。  

>注意力机制与Seq2seq模型  

注意力机制中，在Dot-product Attention中，key与query维度需要一致，在MLP Attention中则不需要。  
注意力机制本身有高效的并行性，但引入注意力并不能改变seq2seq内部RNN的迭代机制，因此无法加速。  
seq2seq模型的预测需人为设定终止条件，设定最长序列长度或者输出[EOS]结束符号，若不加以限制则可能生成无穷长度序列。  
>Transformer  

Multiheadattention层。参考MultiHeadAttention模块的定义。h个注意力头中，每个的参数量为3d^2，最后的输出层形状为hd×d，所以参数量共为4hd^24。  
PositionWiseFFN，基于位置的前馈网络层，主要作用在于增加容量。
add and norm 层，平滑整合输入和其他层的输出。

