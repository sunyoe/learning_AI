*《动手学深度学习》第三次笔记  以NLP方向为主*  
***    
***task6***
>**批量归一化和残差网络**  
Resnet网络和Desnet网络的区别在于：前者最后是相加，两个结果的大小需要一致，如果不一致则使用一些方法修改大小；desnet则是合并成一个新的向量，因此两个结果的通道数可以不一样。  
如果使用nn元语法模型存在数据稀疏问题，最终计算出来的大部分参数都是0.  
>**梯度下降**  
随机梯度下降的时间复杂度是O(1).  
牛顿法相比梯度下降的一个优势在于：梯度下降“步幅”的确定比较困难，而牛顿法相当于可以通过Hessian矩阵来调整“步幅”。牛顿法需要计算Hessian矩阵的逆，计算量比较大。在牛顿法中，局部极小值也可以通过调整学习率来解决。  
可以通过修改 train_sgd 函数的参数batch_size来分别使用梯度下降、随机梯度下降和小批量随机梯度下降。   
